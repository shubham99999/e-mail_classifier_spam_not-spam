{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR='enron'\n",
    "target_names=['ham','spam']\n",
    "\n",
    "def get_data(DATA_DIR):\n",
    "    subfolders=['enron%d'% i for i in range(1,7)]\n",
    "    \n",
    "    data=[]\n",
    "    target=[]\n",
    "    for subfolder in subfolders:\n",
    "        \n",
    "        #spam\n",
    "        spam_files=os.listdir(os.path.join(DATA_DIR,subfolder,'spam'))# returns a list containing names of entries in directory given by path\n",
    "        for spam_file in spam_files:\n",
    "            with open(os.path.join(DATA_DIR,subfolder,'spam',spam_file),encoding=\"latin-1\") as f:\n",
    "                data.append(f.read())\n",
    "                target.append(1)\n",
    "        \n",
    "        #ham\n",
    "        ham_files=os.listdir(os.path.join(DATA_DIR,subfolder,'ham'))\n",
    "        for ham_file in ham_files:\n",
    "            with open(os.path.join(DATA_DIR,subfolder,'ham',ham_file),encoding=\"latin-1\") as f:\n",
    "                data.append(f.read())\n",
    "                target.append(0)\n",
    "    \n",
    "    return data,target       \n",
    "# data consists of the text of the email and target consists of either 0(not spam) or 1(spam)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SpamDetector(object):\n",
    "    def clean(self,s): # cleans the string by removing punctuations\n",
    "        translator=str.maketrans(\"\",\"\",string.punctuation)\n",
    "        return s.translate(translator)\n",
    "    \n",
    "    def tokenize(self,text): # tokenizes our string into words\n",
    "        text=self.clean(text).lower()\n",
    "        return re.split(\"\\W+\",text)\n",
    "    \n",
    "    def stop_word_remover(self,s): # removes stop words \n",
    "        relevant=[]\n",
    "        stop_words=['a','an','the','in','this','that','they','you','we']\n",
    "        new= self.tokenize(s)\n",
    "        for word in new:\n",
    "            if word not in stop_words:\n",
    "                relevant.append(word)\n",
    "        return relevant     \n",
    "    \n",
    "    def get_word_counts(self,words): # how many times each word appears in a list of words\n",
    "        word_counts={}\n",
    "        for word in words:\n",
    "            word_counts[word]=word_counts.get(word,0.0) +1.0\n",
    "        return word_counts    \n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        n = len(X)\n",
    "        self.log_class_priors['spam'] = math.log(sum(1 for label in Y if label == 1) / n)\n",
    "        self.log_class_priors['ham'] = math.log(sum(1 for label in Y if label == 0) / n)\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 1 else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "        \n",
    "    \n",
    "    def predict(self,X):\n",
    "        result=[]\n",
    "        for x in X:\n",
    "            counts=self.get_word_counts(self.stop_word_remover(x))\n",
    "            \n",
    "            spam_score=0\n",
    "            ham_score=0\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "            \n",
    "                # laplace correction\n",
    "                log_w_given_spam=math.log((self.word_counts['spam'].get(word,0.0)+1)/(sum(self.word_counts['spam'].values())+len(self.vocab)))\n",
    "                log_w_given_ham=math.log((self.word_counts['ham'].get(word,0.0)+1)/(sum(self.word_counts['ham'].values())+len(self.vocab)))\n",
    "            \n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    "        \n",
    "            if spam_score > ham_score:\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "    \n",
    "        return result                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    X,y=get_data(DATA_DIR)\n",
    "    MNB= SpamDetector()\n",
    "    MNB.fit(X[100:],y[100:])\n",
    "    \n",
    "    pred = MNB.predict(X[:100])\n",
    "    true=y[:100]\n",
    "    \n",
    "    accuracy=sum(1 for i in range(len(pred)) if pred[i]==true[i])/ float(len(pred))\n",
    "    print(\"{0:.4f}\".format(accuracy))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
